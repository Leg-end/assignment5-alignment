import os
# os.environ["HF_ENDPOINT"] = 'https:/hf-mirror.com'
os.environ["HF_HOME"] = "/data/lanyun/worksapce/assignment5-alignment/models"
from vllm import LLM, SamplingParams, RequestOutput
from typing import Callable
from xopen import xopen
from openai import OpenAI
from collections import defaultdict
import json
import argparse
import logging


def prepare_args():
    parser = argparse.ArgumentParser(description="Collect model output for alpaca eval")
    # 添加参数
    parser.add_argument("--mode", choices=["zero-shot", "sft", "dpo"], help="zero-shot, sft or dpo")
    parser.add_argument("--model_name", help="e.g. qwen3-0.6B-base", default="qwen3-0.6B-base")
    parser.add_argument("--model_path", help="e.g. Qwen/Qwen3-0.6B-Base, default at ./models/", default="Qwen/Qwen3-0.6B-Base")

    # 解析参数
    args = parser.parse_args()
    return args


def log_generation(request_outputs: list[RequestOutput],
                   answers: list[str] | None = None,
                   reward_info: list[dict[str, float]] | None = None,
                   prefix: str = "eval",
                   log_num: int | None = 1):
    """
    1. The input prompt.
    2. The response generated by the SFT/RL model.
    3. The ground-truth answer.
    4. The reward information, including format, answer, and total reward.
    5. The average token entropy of the response.
    6. The average response length, average response length for correct responses, and average response length
        for incorrect responses.
    """
    if answers is not None:
        assert len(request_outputs) == len(answers), "The number of request outputs and answers should be the same."
    if reward_info is not None:
        assert len(request_outputs) == len(reward_info), "The number of request outputs and reward info should be the same."
    batch_size = len(request_outputs)
    if log_num is None:
        log_num = batch_size
    log_num = max(1, min(log_num, batch_size))
    avg_response_len = 0
    total_loss = 0
    n_tokens = 0
    gen_results = []
    logging.info("Generation Log".center(80, "="))
    for idx, request_output in enumerate(request_outputs):
        output = request_output.outputs[0]
        response = output.text.strip()
        n_tokens += len(output.token_ids)
        cumulative_ce = -output.cumulative_logprob
        total_loss += cumulative_ce
        response_avg_token_entropy = cumulative_ce / (len(output.token_ids) + 1e-8)
        avg_response_len += len(response.split(' '))
        if reward_info is not None:
            reward = reward_info[idx]
        else:
            reward = {}
        if idx < log_num:
            logging.info(f"Generation result of batch [{idx+1}/{batch_size}] (log_num={log_num}):")
            logging.info(f"Prompt: {request_output.prompt}")
            logging.info(f"Generated response: {response}")
            if answers is not None:
                logging.info(f"Ground-truth answer: {answers[idx]}")
            if reward_info is not None:
                logging.info(f"Reward: {reward_info}")
            logging.info(f"Response's average token entropy: {response_avg_token_entropy:.2f}")
        gen_results.append({"prompt": request_output.prompt, "answer": answers[idx], "response": response, **reward})
    avg_response_len /= batch_size
    logging.info(f"Average response length: {avg_response_len:.2f}")
    return gen_results, {f"{prefix}/n_tokens": n_tokens,
                         f"{prefix}/avg_response_len": avg_response_len,
                         f"{prefix}/avg_token_entropy": total_loss / (n_tokens + 1e-8),
                         f"{prefix}/total_loss": total_loss}


def evaluate_vllm(
    vllm_model: LLM,
    prompts: list[str],
    gt_answers: list[str],
    eval_sampling_params: SamplingParams,
    output_path: str | None = None,
    log_num: int | None = 1,
    reward_fn: Callable[[str, str], dict[str, float]] | None = None,
    **reward_fn_kwargs
) -> None:
    """
    Evaluate a language model on a list of prompts,
    compute evaluation metrics, and serialize results to disk.
    vllm_model:
        • Qwen 2.5 Math 1.5B Base (for reasoning experiments):
        /data/a5-alignment/models/Qwen2.5-Math-1.5B
        • Llama 3.1 8B Base (for optional instruction tuning experiments):
        /data/a5-alignment/models/Llama-3.1-8B
        • Llama 3.3 70B Instruct (for optional instruction tuning experiments):
        /data/a5-alignment/models/Llama-3.3-70B-Instruct
    """
    request_outputs: list[RequestOutput] = vllm_model.generate(prompts, eval_sampling_params)
    if reward_fn is not None:
        reward_info = []
        for request_output, answer in zip(request_outputs, gt_answers):
            reward_info.append(reward_fn(request_output.outputs[0].text.strip(), answer, **reward_fn_kwargs))
    else:
        reward_info = None
    gen_results, metadata = log_generation(
        request_outputs, answers=gt_answers, reward_info=reward_info,
        log_num=log_num, prefix="eval")
    if output_path is not None:
        with xopen(output_path, "w") as f:
            for info in gen_results:
                f.write(json.dumps(info) + '\n')
    return metadata
                

def evaluate(prompts: list[str],
             answers: list[str],
             model_name_or_path: str,
             output_dir: str,
             stop: list[str],
             mode: str = "zero-shot",
             reward_fn: Callable[[str, str], dict[str, float]] | None = None,
             **reward_fn_kwargs):
    if os.path.isfile(model_name_or_path):
        model_name = os.path.splitext(os.path.basename(model_name_or_path))[0]
    else:
        model_name = model_name_or_path
    model = LLM(model=model_name_or_path)
    sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=1024, stop=stop,
                                     include_stop_str_in_output=True, logprobs=1)
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, mode, model_name, "eval_result.jsonl")
    evaluate_vllm(vllm_model=model,
                  reward_fn=reward_fn,
                  prompts=prompts,
                  gt_answers=answers,
                  eval_sampling_params=sampling_params,
                  output_path=output_path,
                  **reward_fn_kwargs)
    

def evaluate_reward(response_path: str,
                    reward_fn: Callable[[str, str], dict[str, float]],
                    **reward_fn_kwargs):
    with xopen(response_path, "r") as f, xopen(response_path.replace(".jsonl", "_reward.jsonl"), "w") as fw:
        for line in f:
            data = json.loads(line)
            response = data["response"]
            answer = data["answer"]
            reward_dict = reward_fn(response, answer, **reward_fn_kwargs)
            data.update(reward_dict)
            fw.write(json.dumps(data) + '\n')


def evaluate_remote_llm_reward(api_key: str,
                               base_url: str,
                               model_name: str,
                               response_path1: str,
                               response_path2: str):
    with xopen("./prompts/scorer.prompt") as f:
        instruction = f.read()
    client = OpenAI(api_key=api_key, base_url=base_url)
    model_1_win = 0
    model_2_win = 0
    fail_parse = 0
    
    def parse(response):
        """
        Return the rank of model_1, e.g. the first model
        """
        try:
            eval_obj = eval(response)
            if not isinstance(eval_obj, list):
                return None
            if isinstance(eval_obj, dict):
                ranking_list = ['ranking']
            elif isinstance(eval_obj, list):
                ranking_list = eval_obj
            for x in ranking_list:
                if x['model'] == 'model_1':
                    return x['rank']
        except:
            return None

        return None
    with open(response_path1) as f1, open(response_path2) as f2:
        for lines in zip(f1, f2):
            outputs = []
            for i, line in enumerate(lines):
                response = json.loads(line)["response"]
                outputs.append({"model": f"model_{i}", "answer": response})
            outputs_str = json.dumps(outputs, indent=4)
            content = instruction.format(outputs=outputs_str)
            response = client.Completion.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": "Your are a helpful assistant"},
                    {"role": "user", "content": content}
                ],
                stream=False
            )
            preference_response = response.choices[0].message.content
            model_1_rank = parse(preference_response)
        
            print("Instruction:")
            print(instruction)
            
            print("output of model 1:")
            print(lines[0])
            
            print("output of model 2:")
            print(lines[1])
            
            print("Response from deepseek:")
            print(preference_response)
            
            print("Rank of model 1:")
            print(model_1_rank)
            
            # break
            
            if model_1_rank == 1:
                model_1_win += 1
            elif model_1_rank == 2:
                model_2_win += 1
            elif model_1_rank == None:
                fail_parse += 1